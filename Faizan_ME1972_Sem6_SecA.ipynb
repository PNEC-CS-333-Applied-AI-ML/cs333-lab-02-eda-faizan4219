{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "_MC-mM9DsJuz",
        "outputId": "7acf6389-90dd-4d60-e268-e1db56915d55"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'milling_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1316479138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 2. Load Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"milling_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Make sure 'milling_dataset.csv' is uploaded to your Colab environment or specify the correct file path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFirst 5 Rows:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'milling_dataset.csv'"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# LAB 02 – EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# Milling Dataset\n",
        "# ============================================\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Import Libraries\n",
        "# -------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Load Dataset\n",
        "# -------------------------------\n",
        "df = pd.read_csv(\"milling_dataset.csv\")   # Make sure 'milling_dataset.csv' is uploaded to your Colab environment or specify the correct file path.\n",
        "\n",
        "print(\"\\nFirst 5 Rows:\\n\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nLast 5 Rows:\\n\")\n",
        "print(df.tail())\n",
        "\n",
        "print(\"\\nData Types:\\n\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nDataset Shape (Rows, Columns):\\n\")\n",
        "print(df.shape)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# TASK 2 – DATA CLEANING\n",
        "# ============================================\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Missing Values\n",
        "# -------------------------------\n",
        "print(\"\\nMissing Values Before Cleaning:\\n\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Fill numeric columns with median\n",
        "for col in df.select_dtypes(include=np.number).columns:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "# Fill categorical columns with mode\n",
        "for col in df.select_dtypes(include=\"object\").columns:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "print(\"\\nMissing Values After Cleaning:\\n\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Remove Duplicates\n",
        "# -------------------------------\n",
        "print(\"\\nDuplicate Rows:\", df.duplicated().sum())\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(\"Shape After Removing Duplicates:\", df.shape)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# TASK 3 – STATISTICAL ANALYSIS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n========== CENTER ==========\")\n",
        "\n",
        "mean_vals = df.mean(numeric_only=True)\n",
        "median_vals = df.median(numeric_only=True)\n",
        "mode_vals = df.mode(numeric_only=True)\n",
        "\n",
        "print(\"\\nMean:\\n\", mean_vals)\n",
        "print(\"\\nMedian:\\n\", median_vals)\n",
        "print(\"\\nMode:\\n\", mode_vals.head())\n",
        "\n",
        "\n",
        "print(\"\\n========== SPREAD ==========\")\n",
        "\n",
        "variance = df.var(numeric_only=True)\n",
        "range_vals = df.max(numeric_only=True) - df.min(numeric_only=True)\n",
        "\n",
        "Q1 = df.quantile(0.25, numeric_only=True)\n",
        "Q3 = df.quantile(0.75, numeric_only=True)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print(\"\\nVariance:\\n\", variance)\n",
        "print(\"\\nRange:\\n\", range_vals)\n",
        "print(\"\\nIQR:\\n\", IQR)\n",
        "\n",
        "highest_variability = variance.idxmax()\n",
        "print(\"\\nSensor with Highest Variability:\", highest_variability)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# DISTRIBUTION ANALYSIS (Tool Wear)\n",
        "# ============================================\n",
        "\n",
        "if \"Tool_Wear\" in df.columns:\n",
        "\n",
        "    print(\"\\nAnalyzing Tool_Wear Distribution...\")\n",
        "\n",
        "    # Histogram\n",
        "    plt.figure()\n",
        "    plt.hist(df[\"Tool_Wear\"], bins=30)\n",
        "    plt.title(\"Histogram of Tool Wear\")\n",
        "    plt.xlabel(\"Tool Wear\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "    # KDE\n",
        "    plt.figure()\n",
        "    sns.kdeplot(df[\"Tool_Wear\"], fill=True)\n",
        "    plt.title(\"KDE Plot of Tool Wear\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Mean Tool Wear:\", df[\"Tool_Wear\"].mean())\n",
        "    print(\"Median Tool Wear:\", df[\"Tool_Wear\"].median())\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# OUTLIER DETECTION (IQR Method)\n",
        "# ============================================\n",
        "\n",
        "if \"Tool_Wear\" in df.columns:\n",
        "\n",
        "    Q1 = df[\"Tool_Wear\"].quantile(0.25)\n",
        "    Q3 = df[\"Tool_Wear\"].quantile(0.75)\n",
        "    IQR_value = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR_value\n",
        "    upper_bound = Q3 + 1.5 * IQR_value\n",
        "\n",
        "    outliers = df[(df[\"Tool_Wear\"] < lower_bound) | (df[\"Tool_Wear\"] > upper_bound)]\n",
        "\n",
        "    print(\"\\nNumber of Outliers (IQR Method):\", len(outliers))\n",
        "\n",
        "    # Remove outliers\n",
        "    df_clean = df[(df[\"Tool_Wear\"] >= lower_bound) & (df[\"Tool_Wear\"] <= upper_bound)]\n",
        "\n",
        "    print(\"Shape After Removing Outliers:\", df_clean.shape)\n",
        "\n",
        "else:\n",
        "    df_clean = df.copy()\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# TASK 4 – CORRELATION ANALYSIS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n========== CORRELATION MATRIX ==========\")\n",
        "\n",
        "corr_matrix = df_clean.corr(numeric_only=True)\n",
        "print(corr_matrix)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "if \"Tool_Wear\" in corr_matrix.columns:\n",
        "    tool_corr = corr_matrix[\"Tool_Wear\"].abs().sort_values(ascending=False)\n",
        "    print(\"\\nCorrelation with Tool_Wear:\\n\")\n",
        "    print(tool_corr)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# TASK 5 – SCATTER PLOT\n",
        "# ============================================\n",
        "\n",
        "if \"Tool_Wear\" in df_clean.columns:\n",
        "\n",
        "    numeric_cols = df_clean.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Try plotting against the first numeric feature except Tool_Wear\n",
        "    for col in numeric_cols:\n",
        "        if col != \"Tool_Wear\":\n",
        "            plt.figure()\n",
        "            plt.scatter(df_clean[col], df_clean[\"Tool_Wear\"])\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel(\"Tool Wear\")\n",
        "            plt.title(f\"Tool Wear vs {col}\")\n",
        "            plt.show()\n",
        "            break\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# ENGINEERING INTERPRETATION (AUTO PRINT)\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n========== ENGINEERING INSIGHTS ==========\")\n",
        "\n",
        "if \"Tool_Wear\" in df_clean.columns:\n",
        "    print(\"• Tool wear tends to increase with features that show strong positive correlation.\")\n",
        "    print(\"• The most predictive feature is the one with highest absolute correlation.\")\n",
        "    print(\"• The most stable sensor is the one with lowest variance.\")\n",
        "    print(\"• Tool wear can be predicted using regression or machine learning models.\")\n",
        "else:\n",
        "    print(\"Tool_Wear column not found. Check dataset column names.\")"
      ]
    }
  ]
}